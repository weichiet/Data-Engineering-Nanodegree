{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Engineering Capstone Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Summary\n",
    "The objective of this project is to collect data related to immigration and build a simple data warehouse where it can be further analysed in a fast and efficient manner. \n",
    "\n",
    "Project Files:  \n",
    "* `Capstone_Project.ipynb`: Current notebook. It documents all the steps taken in this project.  \n",
    "* `Creating_Data_Warehouse.ipynb`: This notebook illustrates the step to create a new Amazon Redshift cluster.  \n",
    "* `Data_Dictionary.ipynb`: This notebook contains the data dictionary of the data model of this project.  \n",
    "* `etl.py`, `sql_queries.py` and `dwh.cfg`: The Python and configuration files for ETL process.\n",
    "* `data_model.png`: The diagram of the data model.\n",
    "* `raw_data` folder: The raw dataset provided by Udacity for this project.\n",
    "* `csv_files` folder: List of csv files of the dataset after cleaning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Scope the Project and Gather Data\n",
    "\n",
    "The process will first use Spark and Pandas to read, parse and clean the data that are stored in local directory. The data will then be transferred to Amazon S3 and ready to be loaded by Amazon Redshift. The final solution consists of several tables of U.S. immigration information that could be accessed for further analysis by using SQL.\n",
    "\n",
    "The data in this project consists of information related to immigration in the United States:\n",
    "* I94 Immigration Data - This data was from the US National Tourism and Trade Office. The data contains international visitor arrival statistics by world regions, and select countries. The data contains the type of visa, the mode of transportation, the age groups, states visited, and the top ports of entry for immigration into the United States. The data was collected from [here](https://travel.trade.gov/research/reports/i94/historical/2016.html).\n",
    "* U.S. City Demographic Data - This dataset contains information about the demographics of all US cities, and census-designated places with a population greater or equal to 65,000. The dataset can be accessed [here](https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/).\n",
    "* Airport Code Table - This dataset contains a simple table of airport codes, and corresponding cities. The data can be accessed [here](https://datahub.io/core/airport-codes#data).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import json\n",
    "import psycopg2\n",
    "import pandas.io.sql as sqlio\n",
    "import configparser\n",
    "from datetime import datetime\n",
    "\n",
    "import os\n",
    "import pyspark.sql\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DateType\n",
    "from pyspark.sql.functions import expr , udf ,trim ,year, month, dayofmonth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load parameters from configuration files\n",
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('dwh.cfg'))\n",
    "\n",
    "KEY = config.get('AWS','KEY')\n",
    "SECRET = config.get('AWS','SECRET')\n",
    "S3_BUCKET = config.get('S3', 'S3_BUCKET')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load the data that are stored in local directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load immigrations data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of immigration data in the folder\n",
    "immi_path = '../../data/18-83510-I94-Data-2016/'\n",
    "immig_files = [os.path.join(immi_path, f) for f in os.listdir(immi_path)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The immigration data for Jun 2016 (`i94_jun16_sub.sas7bdat`) consists of extra columns, namely *'validres','delete_days','delete_mexl','delete_dup','delete_recdup'* and *'delete_visa'*. These columns are dropped before joining with other immigration data of another months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read 'i94_jun16_sub.sas7bdat' and drop extra columns\n",
    "df_immig = spark.read.format('com.github.saurfang.sas.spark') \\\n",
    "                .option(\"inferSchema\", \"true\") \\\n",
    "                .option(\"dateFormat\", \"yyyyMMdd\") \\\n",
    "                .load(immig_files[4]) \\\n",
    "                .drop('validres','delete_days','delete_mexl','delete_dup','delete_recdup','delete_visa')\n",
    "\n",
    "# Remove 'i94_jun16_sub.sas7bdat' from file list since it has been read\n",
    "immig_files.pop(4);\n",
    "\n",
    "# Combine with the rest of the SAS files\n",
    "for i in range(0, len(immig_files)):\n",
    "    df = spark.read.format('com.github.saurfang.sas.spark') \\\n",
    "                .option(\"inferSchema\", \"true\") \\\n",
    "                .option(\"dateFormat\", \"yyyyMMdd\") \\\n",
    "                .load(immig_files[i])\n",
    "\n",
    "    df_immig = df_immig.union(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The descriptor file, `I94_SAS_Labels_Descriptions.SAS` consists of data dictionary of the immigration data. We parse it manually and save the information as several csv files in `./csv_files/` folder. The following files are the results of the manually parsing and will be loaded as dimension tables in the later ETL process:\n",
    "* `arrival_mode.csv`\n",
    "* `countries.csv`\n",
    "* `ports.csv`\n",
    "* `states.csv`\n",
    "* `visa_type.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load demographics data and airport codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load demographics data and airport_codes csv \n",
    "df_demog = pd.read_csv('./raw_data/us-cities-demographics.csv', delimiter=';')\n",
    "df_port = pd.read_csv('./raw_data/airport-codes_csv.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Explore and Assess the Data\n",
    "\n",
    "Next, we will explore the data and apply necessary step to clean the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Immigrations Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40790529"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of rows\n",
    "df_immig.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+\n",
      "|cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|         admnum|fltno|visatype|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+\n",
      "|  4.0|2016.0|   6.0| 135.0| 135.0|    XXX|20612.0|   null|   null|   null|  59.0|    2.0|  1.0|    null|    null| null|      Z|   null|      U|   null| 1957.0|10032016|  null|  null|   null|1.4938462027E10| null|      WT|\n",
      "|  5.0|2016.0|   6.0| 135.0| 135.0|    XXX|20612.0|   null|   null|   null|  50.0|    2.0|  1.0|    null|    null| null|      Z|   null|      U|   null| 1966.0|10032016|  null|  null|   null|1.7460063727E10| null|      WT|\n",
      "|  6.0|2016.0|   6.0| 213.0| 213.0|    XXX|20609.0|   null|   null|   null|  27.0|    3.0|  1.0|    null|    null| null|      T|   null|      U|   null| 1989.0|     D/S|  null|  null|   null|  1.679297785E9| null|      F1|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display a few rows of data\n",
    "df_immig.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Some columns such as *'i94yr'*, *'i94mon'* and *'i94bir'* are stored as floating point. We will cast it to integer type.\n",
    "* The various dates in the data are stored as SAS date format, which is the number of days since 1/1/1960. We will transform them to date type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: long (nullable = true)\n",
      " |-- i94yr: integer (nullable = true)\n",
      " |-- i94mon: integer (nullable = true)\n",
      " |-- i94cit: integer (nullable = true)\n",
      " |-- i94res: integer (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: date (nullable = true)\n",
      " |-- i94mode: integer (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: date (nullable = true)\n",
      " |-- i94bir: integer (nullable = true)\n",
      " |-- i94visa: integer (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: integer (nullable = true)\n",
      " |-- dtaddto: date (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Update data type of several columns\n",
    "df_immig = df_immig.withColumn('cicid', df_immig[\"cicid\"].cast(\"bigint\")) \\\n",
    "                .withColumn('i94yr', df_immig[\"i94yr\"].cast(\"int\"))\\\n",
    "                .withColumn('i94mon', df_immig[\"i94mon\"].cast(\"int\"))\\\n",
    "                .withColumn('i94cit', df_immig[\"i94cit\"].cast(\"int\"))\\\n",
    "                .withColumn('i94res', df_immig[\"i94res\"].cast(\"int\"))\\\n",
    "                .withColumn('i94mode', df_immig[\"i94mode\"].cast(\"int\"))\\\n",
    "                .withColumn('i94bir', df_immig[\"i94bir\"].cast(\"int\"))\\\n",
    "                .withColumn('i94visa', df_immig[\"i94visa\"].cast(\"int\"))\\\n",
    "                .withColumn('count', df_immig[\"count\"].cast(\"int\"))\\\n",
    "                .withColumn('biryear', df_immig[\"biryear\"].cast(\"int\"))\\\n",
    "                .withColumn('arrdate', expr(\"date_add(to_date('1960-01-01'), arrdate)\")) \\\n",
    "                .withColumn('depdate', expr(\"date_add(to_date('1960-01-01'), depdate)\")) \\\n",
    "                .withColumn('dtaddto', expr(\"to_date(dtaddto,'MMddyyyy')\"))\n",
    "\n",
    "df_immig.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------+------+------+-------+----------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+----------+------+------+-------+---------------+-----+--------+\n",
      "|cicid|i94yr|i94mon|i94cit|i94res|i94port|   arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear|   dtaddto|gender|insnum|airline|         admnum|fltno|visatype|\n",
      "+-----+-----+------+------+------+-------+----------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+----------+------+------+-------+---------------+-----+--------+\n",
      "|   22| 2016|     8|   323|   323|    NYC|2016-08-01|      1|     FL|   null|    23|      3|    1|20160801|     RID| null|      U|   null|   null|   null|   1993|      null|     M|  null|     EK| 6.451049563E10|  201|      F1|\n",
      "|   55| 2016|     8|   209|   209|    AGA|2016-08-01|      1|     CA|   null|    41|      2|    1|20160801|    null| null|      A|   null|   null|   null|   1975|2016-09-14|     M|  3955|     JL|5.7571868933E10|00941|     GMT|\n",
      "|   56| 2016|     8|   209|   209|    AGA|2016-08-01|      1|     GU|   null|    24|      2|    1|20160801|    null| null|      A|   null|   null|   null|   1992|2016-09-15|     F|  3661|     UA|5.7571894533E10|00874|     GMT|\n",
      "+-----+-----+------+------+------+-------+----------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+----------+------+------+-------+---------------+-----+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display a few row of the cleaned data\n",
    "df_immig.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The processed immigration data will be stored as parquet files and transfer to a Amazon S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the immigrations data as parquet\n",
    "df_immig.write.mode(\"overwrite\").parquet(\"immigrations_parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer the parquet files to S3 bucket\n",
    "\n",
    "session = boto3.Session(\n",
    "                aws_access_key_id = KEY,\n",
    "                aws_secret_access_key = SECRET)\n",
    "\n",
    "s3 = session.resource('s3')\n",
    "bucket = s3.Bucket(S3_BUCKET)\n",
    "\n",
    "path = './immigrations_parquet/'\n",
    "for subdir, dirs, files in os.walk(path):\n",
    "    for file in files:\n",
    "        file_name, file_extension = os.path.splitext(file)\n",
    "        full_path = os.path.join(subdir, file)\n",
    "        if file_extension == '.' + 'parquet':            \n",
    "            with open(full_path, 'rb') as data:\n",
    "                bucket.put_object(Key = 'immigrations_parquet/' + full_path[len(path):], Body=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Demographics Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.8</td>\n",
       "      <td>40601.0</td>\n",
       "      <td>41862.0</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562.0</td>\n",
       "      <td>30908.0</td>\n",
       "      <td>2.60</td>\n",
       "      <td>MD</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>25924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quincy</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>41.0</td>\n",
       "      <td>44129.0</td>\n",
       "      <td>49500.0</td>\n",
       "      <td>93629</td>\n",
       "      <td>4147.0</td>\n",
       "      <td>32935.0</td>\n",
       "      <td>2.39</td>\n",
       "      <td>MA</td>\n",
       "      <td>White</td>\n",
       "      <td>58723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoover</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>38.5</td>\n",
       "      <td>38040.0</td>\n",
       "      <td>46799.0</td>\n",
       "      <td>84839</td>\n",
       "      <td>4819.0</td>\n",
       "      <td>8229.0</td>\n",
       "      <td>2.58</td>\n",
       "      <td>AL</td>\n",
       "      <td>Asian</td>\n",
       "      <td>4759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rancho Cucamonga</td>\n",
       "      <td>California</td>\n",
       "      <td>34.5</td>\n",
       "      <td>88127.0</td>\n",
       "      <td>87105.0</td>\n",
       "      <td>175232</td>\n",
       "      <td>5821.0</td>\n",
       "      <td>33878.0</td>\n",
       "      <td>3.18</td>\n",
       "      <td>CA</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>24437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newark</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>34.6</td>\n",
       "      <td>138040.0</td>\n",
       "      <td>143873.0</td>\n",
       "      <td>281913</td>\n",
       "      <td>5829.0</td>\n",
       "      <td>86253.0</td>\n",
       "      <td>2.73</td>\n",
       "      <td>NJ</td>\n",
       "      <td>White</td>\n",
       "      <td>76402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               City          State  Median Age  Male Population  \\\n",
       "0     Silver Spring       Maryland        33.8          40601.0   \n",
       "1            Quincy  Massachusetts        41.0          44129.0   \n",
       "2            Hoover        Alabama        38.5          38040.0   \n",
       "3  Rancho Cucamonga     California        34.5          88127.0   \n",
       "4            Newark     New Jersey        34.6         138040.0   \n",
       "\n",
       "   Female Population  Total Population  Number of Veterans  Foreign-born  \\\n",
       "0            41862.0             82463              1562.0       30908.0   \n",
       "1            49500.0             93629              4147.0       32935.0   \n",
       "2            46799.0             84839              4819.0        8229.0   \n",
       "3            87105.0            175232              5821.0       33878.0   \n",
       "4           143873.0            281913              5829.0       86253.0   \n",
       "\n",
       "   Average Household Size State Code                       Race  Count  \n",
       "0                    2.60         MD         Hispanic or Latino  25924  \n",
       "1                    2.39         MA                      White  58723  \n",
       "2                    2.58         AL                      Asian   4759  \n",
       "3                    3.18         CA  Black or African-American  24437  \n",
       "4                    2.73         NJ                      White  76402  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display a few rows of data\n",
    "df_demog.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2891, 12)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the dimension of the data\n",
    "df_demog.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no changes to the demographics data. Nevertheless, we store the data as csv file in `./csv_files` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the data as csv file\n",
    "df_demog.to_csv('./csv_files/us-cities-demographics.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Airport Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ident</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>elevation_ft</th>\n",
       "      <th>continent</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>iso_region</th>\n",
       "      <th>municipality</th>\n",
       "      <th>gps_code</th>\n",
       "      <th>iata_code</th>\n",
       "      <th>local_code</th>\n",
       "      <th>coordinates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00A</td>\n",
       "      <td>heliport</td>\n",
       "      <td>Total Rf Heliport</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-PA</td>\n",
       "      <td>Bensalem</td>\n",
       "      <td>00A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00A</td>\n",
       "      <td>-74.93360137939453, 40.07080078125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00AA</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Aero B Ranch Airport</td>\n",
       "      <td>3435.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-KS</td>\n",
       "      <td>Leoti</td>\n",
       "      <td>00AA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AA</td>\n",
       "      <td>-101.473911, 38.704022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00AK</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Lowell Field</td>\n",
       "      <td>450.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AK</td>\n",
       "      <td>Anchor Point</td>\n",
       "      <td>00AK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AK</td>\n",
       "      <td>-151.695999146, 59.94919968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00AL</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Epps Airpark</td>\n",
       "      <td>820.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AL</td>\n",
       "      <td>Harvest</td>\n",
       "      <td>00AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AL</td>\n",
       "      <td>-86.77030181884766, 34.86479949951172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00AR</td>\n",
       "      <td>closed</td>\n",
       "      <td>Newport Hospital &amp; Clinic Heliport</td>\n",
       "      <td>237.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AR</td>\n",
       "      <td>Newport</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-91.254898, 35.6087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ident           type                                name  elevation_ft  \\\n",
       "0   00A       heliport                   Total Rf Heliport          11.0   \n",
       "1  00AA  small_airport                Aero B Ranch Airport        3435.0   \n",
       "2  00AK  small_airport                        Lowell Field         450.0   \n",
       "3  00AL  small_airport                        Epps Airpark         820.0   \n",
       "4  00AR         closed  Newport Hospital & Clinic Heliport         237.0   \n",
       "\n",
       "  continent iso_country iso_region  municipality gps_code iata_code  \\\n",
       "0       NaN          US      US-PA      Bensalem      00A       NaN   \n",
       "1       NaN          US      US-KS         Leoti     00AA       NaN   \n",
       "2       NaN          US      US-AK  Anchor Point     00AK       NaN   \n",
       "3       NaN          US      US-AL       Harvest     00AL       NaN   \n",
       "4       NaN          US      US-AR       Newport      NaN       NaN   \n",
       "\n",
       "  local_code                            coordinates  \n",
       "0        00A     -74.93360137939453, 40.07080078125  \n",
       "1       00AA                 -101.473911, 38.704022  \n",
       "2       00AK            -151.695999146, 59.94919968  \n",
       "3       00AL  -86.77030181884766, 34.86479949951172  \n",
       "4        NaN                    -91.254898, 35.6087  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display a few rows of data\n",
    "df_port.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55075, 12)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the dimension of the data\n",
    "df_port.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For airports located in U.S., the *'iso_region'* column consists of the state code. We will extract the state code so that it could be used to join with other tables in later anaylsis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting column 'iso_region' to extract state code\n",
    "df_port['iso_region'] = df_port['iso_region'].apply(lambda x: x.split('-')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the data as csv file\n",
    "df_port.to_csv('./csv_files/airport-codes.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we transfer all csv files in `./csv_files` folder to S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer all csv files to S3 bucket\n",
    "path = './csv_files/'\n",
    "for subdir, dirs, files in os.walk(path):\n",
    "    for file in files:\n",
    "        file_name, file_extension = os.path.splitext(file)\n",
    "        full_path = os.path.join(subdir, file)\n",
    "        if file_extension == '.' + 'csv':            \n",
    "            with open(full_path, 'rb') as data:\n",
    "                bucket.put_object(Key = full_path[len(path):], Body=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define the Data Model\n",
    "### 3.1 Conceptual Data Model\n",
    "<img src=\"data_model.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Mapping Out Data Pipelines\n",
    "To pipeline the data into the chosen data model:\n",
    "1. Create or connect to an Amazon Redshift Cluster.\n",
    "2. Drop any existing tables.\n",
    "3. Create the tables in the defined data model.\n",
    "4. Load the data from Amazon S3 to the tables.\n",
    "5. Run data quality check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Run Pipelines to Model the Data \n",
    "### 4.1 Create the data model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The necessary steps to create a new Amazon Redshift cluster are outlined in a seperate notebook, `Creating_Data_Warehouse.ipynb`. After creating the Redshift cluster, run `etl.py` to create and load the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database connected\n",
      "Dropping existing tables...\n",
      "All existing tables dropped\n",
      "Creating tables...\n",
      "All tables created\n",
      "Inserting data into tables...\n",
      "All data inserted into tables\n"
     ]
    }
   ],
   "source": [
    "%run etl.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Data Quality Checks\n",
    "To ensure the pipeline ran as expected, we first ensure that the tables are no empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the database\n",
    "conn = psycopg2.connect(\"host={} dbname={} user={} password={} port={}\".format(*config['CLUSTER'].values()))\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensures that for a given table, there exists records\n",
    "\n",
    "table_names = ['immigrations', 'arrival_mode', 'countries', 'ports', 'states', 'visa_type', 'airport_codes', 'demographics']\n",
    "\n",
    "for table in table_names:\n",
    "    sql = 'SELECT COUNT(*) FROM {};'.format(table)\n",
    "    cur.execute(sql)\n",
    "    count = cur.fetchone()\n",
    "\n",
    "    if (count[0] == 0):\n",
    "        raise Exception('There are no records for table {}'.format(table))    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also run a a few SQL enquries to ensure the database is working properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>port_name</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NEW YORK, NY</td>\n",
       "      <td>6058033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MIAMI, FL</td>\n",
       "      <td>4735010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LOS ANGELES, CA</td>\n",
       "      <td>4187439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SAN FRANCISCO, CA</td>\n",
       "      <td>2091507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HONOLULU, HI</td>\n",
       "      <td>2064457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NEWARK/TETERBORO, NJ</td>\n",
       "      <td>1707321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CHICAGO, IL</td>\n",
       "      <td>1615110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ORLANDO, FL</td>\n",
       "      <td>1569443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>HOUSTON, TX</td>\n",
       "      <td>1235650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AGANA, GU</td>\n",
       "      <td>1231022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                port_name    count\n",
       "0  NEW YORK, NY            6058033\n",
       "1  MIAMI, FL               4735010\n",
       "2  LOS ANGELES, CA         4187439\n",
       "3  SAN FRANCISCO, CA       2091507\n",
       "4  HONOLULU, HI            2064457\n",
       "5  NEWARK/TETERBORO, NJ    1707321\n",
       "6  CHICAGO, IL             1615110\n",
       "7  ORLANDO, FL             1569443\n",
       "8  HOUSTON, TX             1235650\n",
       "9  AGANA, GU               1231022"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Top 10 city from where immigrants arrived\n",
    "\n",
    "sql = \"\"\"\n",
    "        SELECT TOP 10 ports.port_name, COUNT(cicid) AS count\n",
    "        FROM immigrations immi \n",
    "        JOIN ports ON immi.port = ports.port_code\n",
    "        GROUP BY ports.port_name\n",
    "        ORDER BY COUNT(cicid) DESC\n",
    "\"\"\"\n",
    "\n",
    "sqlio.read_sql_query(sql, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>total_immigrations</th>\n",
       "      <th>total_foreign_born</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FL</td>\n",
       "      <td>7571672</td>\n",
       "      <td>7845566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NY</td>\n",
       "      <td>6174793</td>\n",
       "      <td>17186873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CA</td>\n",
       "      <td>5928310</td>\n",
       "      <td>37059662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HI</td>\n",
       "      <td>2153992</td>\n",
       "      <td>506560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TX</td>\n",
       "      <td>1545945</td>\n",
       "      <td>14498054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NV</td>\n",
       "      <td>1274193</td>\n",
       "      <td>2406685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>IL</td>\n",
       "      <td>964594</td>\n",
       "      <td>4632600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MA</td>\n",
       "      <td>952068</td>\n",
       "      <td>2573815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NJ</td>\n",
       "      <td>888263</td>\n",
       "      <td>2327750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>WA</td>\n",
       "      <td>733776</td>\n",
       "      <td>2204810</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  state  total_immigrations  total_foreign_born\n",
       "0    FL             7571672             7845566\n",
       "1    NY             6174793            17186873\n",
       "2    CA             5928310            37059662\n",
       "3    HI             2153992              506560\n",
       "4    TX             1545945            14498054\n",
       "5    NV             1274193             2406685\n",
       "6    IL              964594             4632600\n",
       "7    MA              952068             2573815\n",
       "8    NJ              888263             2327750\n",
       "9    WA              733776             2204810"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Comparing the total number of foreign born residents versus total number of arrival immigrations in each state of U.S.\n",
    "\n",
    "sql = \"\"\"\n",
    "        WITH table1 AS (SELECT state_code AS state, SUM(foreign_born) AS total_foreign_born\n",
    "                        FROM demographics\n",
    "                        GROUP BY state_code),\n",
    "              table2 AS (SELECT state, COUNT (cicid) AS total_immigrations\n",
    "                          FROM immigrations\n",
    "                          GROUP BY state)\n",
    "\n",
    "        SELECT table1.state, table2.total_immigrations, table1.total_foreign_born\n",
    "        FROM table1 \n",
    "        JOIN table2 ON table2.state = table1.state\n",
    "        ORDER BY table2.total_immigrations DESC\n",
    "        LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "sqlio.read_sql_query(sql, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>total_immigrations</th>\n",
       "      <th>number_of_airports</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FL</td>\n",
       "      <td>7500392</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NY</td>\n",
       "      <td>5958773</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CA</td>\n",
       "      <td>5828506</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HI</td>\n",
       "      <td>2132758</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TX</td>\n",
       "      <td>1494769</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NV</td>\n",
       "      <td>1264391</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>IL</td>\n",
       "      <td>946468</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MA</td>\n",
       "      <td>910989</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NJ</td>\n",
       "      <td>836683</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>WA</td>\n",
       "      <td>546812</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  state  total_immigrations  number_of_airports\n",
       "0    FL             7500392                  47\n",
       "1    NY             5958773                  26\n",
       "2    CA             5828506                  67\n",
       "3    HI             2132758                  17\n",
       "4    TX             1494769                  56\n",
       "5    NV             1264391                  12\n",
       "6    IL              946468                  16\n",
       "7    MA              910989                  11\n",
       "8    NJ              836683                   9\n",
       "9    WA              546812                  22"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Comparing total number of immigrants (arrived by air) versus total number of airports (large or medium) in each state of U.S.\n",
    "\n",
    "sql = \"\"\"\n",
    "        WITH table1 AS (SELECT iso_region, COUNT(ident) AS number_of_airports\n",
    "                        FROM  airport_codes\n",
    "                        WHERE iso_country = 'US' AND (type = 'large_airport' OR type = 'medium_airport')\n",
    "                        GROUP BY iso_region\n",
    "                        ORDER BY COUNT (ident) DESC),\n",
    "              table2 AS (SELECT state, COUNT (cicid) AS total_immigrations\n",
    "                         FROM immigrations\n",
    "                         WHERE arrival_mode = 1\n",
    "                         GROUP BY state)\n",
    "\n",
    "        SELECT table2.state, table2.total_immigrations, table1.number_of_airports\n",
    "        FROM table1 \n",
    "        JOIN table2 ON table2.state = table1.iso_region\n",
    "        ORDER BY table2.total_immigrations DESC\n",
    "        LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "sqlio.read_sql_query(sql, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Data dictionary \n",
    "\n",
    "The data dictionary of the data model can be found in another notebook, `Data_Dictionary.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tools and Technologies\n",
    "* Spark and Pandas: They are used to load and explore the raw data stored on local directory. Given the data set is considerably small and structured, it can be fit in a local machine memory. Spark can also handle the situation where the size of the data was increase exponentially in the future. \n",
    "* Amazon S3 and Amazon Redshift: We store data in S3 bucket and process it using Redshift cluster. Both tools are scalable and efficient in handling multiple concurrent access."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Update Schedule\n",
    "* *immigrants*: Since thousands of immigrations reach U.S. everyday, this table should be updated daily, provided the data is available.\n",
    "* *demographics*: Since this data is based on U.S. Census Bureau's survey, it should be updated when new data is released by the bureau.\n",
    "* *airport_codes*: As this information doesn't change rapidly, annual update would be an be appropriate choice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scenario Handling\n",
    "\n",
    "* **The data was increased by 100x**  \n",
    "As Amazon Redshift is simple and quickly to scale, we could use a higher specification of Redshift cluster, such as changing the number or type of nodes of the cluster. If the data couldn’t fit in local memory, we could use Spark hosted on Amazon EMR for the data exploration and data cleaning process.\n",
    "* **The data populates a dashboard that must be updated on a daily basis by 7am every day**  \n",
    "We could set up an Apache Airflow pipeline to automate the whole ETL process. The ETL pipeline could be triggered every morning to populate the dashboard.\n",
    "* **The database needed to be accessed by 100+ people**  \n",
    "Amazon Redshift is designed to provides consistently fast performance, even with thousands of concurrent queries. If necessary, we could enable [concurrency scaling feature](https://docs.aws.amazon.com/redshift/latest/dg/concurrency-scaling.html)  of Amazon Redshift, so that additional cluster capacity will be added to process an increase in concurrent read queries. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
